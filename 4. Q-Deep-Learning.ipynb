{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8b5627",
   "metadata": {},
   "source": [
    "# Q-Deep learning\n",
    "\n",
    "Q-learning tradicional con el uso de su tabla tiene limitaciones por ejemplo si los estados y acciones son continuos o muy grandes, porque la tabla se vuelve inmanejable. Para solucionar esto se usa Deep Q-learning, que usa una red neuronal para aproximar la funcion Q(s,a) en lugar de una tabla. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871d158",
   "metadata": {},
   "source": [
    "Remplazamos la tablaa por una funcion no lineal, osea una red neuronal\n",
    "$$\n",
    "Q(s,a; \\theta) \\approx Q^*(s,a)\n",
    "$$\n",
    "\n",
    "- Donde $\\theta$ son los parametros de la red neuronal (pesos y bias).\n",
    "\n",
    "Esto tiene implicaciones en como se calcula la funcion objetivo (tarjet de bellman) y como se calcula la funcion de perdida.\n",
    "\n",
    "Ecuacion para el entrenamiento de la red neuronal:\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\max_{a'} Q_{\\theta^-}(s_{t+1}, a')\n",
    "$$\n",
    "\n",
    "Y la **función de pérdida (error cuadrático medio)** es:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\Big[ \\big( y_t - Q_\\theta(s,a) \\big)^2 \\Big]\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ( $Q_\\theta(s,a)$ ): la **red principal** (online network) que estamos entrenando.\n",
    "* ( $Q_{\\theta^-}(s',a')$ ): la **red objetivo**, con parámetros **congelados** que se actualizan cada cierto número de pasos.\n",
    "* ( $\\mathcal{D}$ ): el **replay buffer**, una memoria de experiencias pasadas.\n",
    "* ( $y_t$ ): el **target de Bellman**, que se considera una estimación del valor óptimo.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33b42a",
   "metadata": {},
   "source": [
    "### Proceso resumido:\n",
    "\n",
    "1. **Recolectar experiencia:**\n",
    "\n",
    "   * El agente interactúa con el entorno y guarda $(s_t, a_t, r_{t+1}, s_{t+1})$ en el buffer ( $\\mathcal{D}$ ).\n",
    "\n",
    "2. **Muestrear minibatch:**\n",
    "   $$\n",
    "   (s_i, a_i, r_i, s'_i) \\sim \\mathcal{D}\n",
    "   $$\n",
    "\n",
    "3. **Calcular los objetivos:**\n",
    "   $$\n",
    "   y_i = r_i + \\gamma \\max_{a'} Q_{\\theta^-}(s'_i, a')\n",
    "   $$\n",
    "\n",
    "4. **Calcular la pérdida:**\n",
    "   $$\n",
    "   L(\\theta) = \\frac{1}{N} \\sum_i (y_i - Q_\\theta(s_i, a_i))^2\n",
    "   $$\n",
    "\n",
    "5. **Actualizar parámetros:**\n",
    "   $$\n",
    "   \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta)\n",
    "   $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
