{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea23366",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### MDP (Markov decision process)\n",
    "\n",
    "Un proceso de Markov esta definido por una 4-tuple $(S, A, P_a, R_a)$, donde:\n",
    "- S: conjunto de estados (State space) discreto o continuo.\n",
    "- A: conjunto de acciones (Action space) discreto o continuo.\n",
    "- $P_a(s,s')$: probabilidad de transicion, es decir, la probabilidad de ir del estado s al estado s' tomando la accion a. $P_a(s,s') = P(s_{t+1}=s' | s_t=s, a_t=a)$.\n",
    "- $R_a(s,s')$: recompensa esperada al ir del estado s al estado s' tomando la accion a. $R_a(s,s') = E[r_{t+1} | s_t=s, a_t=a, s_{t+1}=s']$.\n",
    "- funcion de politica $\\pi$ mapea del state space al action space, es decir, $\\pi(s) = a$.\n",
    "\n",
    "El objetico de un MDP is encontrar una buena politica de decision, una funcion $\\pi$ que especifique la accion a tomar en cada estado s, $\\pi(s)$, de tal manera que maximice la recompensa acumulada a largo plazo. Una vez un MDP este combinado con una politica $\\pi$, se convierte en una cadena de Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac9331",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Objetivo politica $\\pi$\n",
    "\n",
    "**Objetivo de una Política en un MDP**\n",
    "\n",
    "El objetivo de una política ( $\\pi$ ) en un **Proceso de Decisión de Markov (MDP)** es **maximizar la recompensa acumulada esperada** a lo largo del tiempo.\n",
    "\n",
    "\n",
    "- **1. Horizonte Infinito con Descuento**\n",
    "\n",
    "El agente busca maximizar el **valor esperado de las recompensas descontadas**:\n",
    "\n",
    "$$\n",
    "E\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} R_{a_t}(s_t, s_{t+1})\\right]\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "* ( $a_t = \\pi(s_t)$ ): acción elegida por la política.\n",
    "* ( $s_{t+1} \\sim P_{a_t}(s_t, s_{t+1})$ ): transición probabilística.\n",
    "* ( $0 \\le \\gamma \\le 1$ ): **factor de descuento**, que mide la importancia del futuro.\n",
    "\n",
    "  * Si ( $\\gamma$ ) es pequeño → el agente es **más cortoplacista**.\n",
    "  * Si ( $\\gamma$ ) es cercano a 1 → el agente **valora más el futuro**.\n",
    "\n",
    "\n",
    "- **2. Horizonte Fijo de ( H ) Pasos**\n",
    "\n",
    "Alternativamente, se puede usar un **horizonte finito** de ( H ) pasos (sin descuento):\n",
    "\n",
    "$$\n",
    "E\\left[\\sum_{t=0}^{H-1} R_{a_t}(s_t, s_{t+1})\\right]\n",
    "$$\n",
    "\n",
    "Aquí todas las recompensas tienen **igual peso** y solo se consideran los primeros ( H ) pasos.\n",
    "Este enfoque es común en **teoría del aprendizaje**.\n",
    "\n",
    "-  **3. Política Óptima**\n",
    "\n",
    "* Una **política óptima** ( $\\pi^*$ ) es aquella que **maximiza** alguna de las funciones anteriores.\n",
    "* Puede haber **múltiples políticas óptimas** para un mismo MDP.\n",
    "* Por la **propiedad de Markov**, la política óptima depende **solo del estado actual**:\n",
    "  $$a_t = \\pi^*(s_t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fb38d",
   "metadata": {},
   "source": [
    "#### Relación entre autómatas y MDP\n",
    "Como un apunte interesante de cierta foram un MDP puese ser visto con un automata probabilistico, donde no se busca procesar un lenjague por medio de transiciones entre estados, sino que se busca maximizar una recompensa a lo largo de las transiciones entre estados.\n",
    "\n",
    "| **Aspecto**                        | **Autómata Finito Determinista (AFD)**                    | **Proceso de Decisión de Markov (MDP)**                                                      |\n",
    "| ---------------------------------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **Objetivo principal**             | Reconocer o aceptar cadenas de un lenguaje formal.        | Tomar decisiones óptimas en entornos con incertidumbre para maximizar recompensas esperadas. |\n",
    "| **Estructura formal**              | $A = (Q, \\Sigma, \\delta, q_0, F)$                       | $M = (S, A, P, R, \\gamma)$                                                                 |\n",
    "| **Conjunto de estados**            | $Q$: finito.                                            | $S$: puede ser finito o infinito.                                                          |\n",
    "| **Entradas o acciones**            | $\\Sigma$: símbolos del alfabeto.                        | $A$: conjunto de acciones elegibles.                                                       |\n",
    "| **Función de transición**          | $\\delta: Q \\times \\Sigma \\to Q$ (determinista).         | $P(s' \\mid s,a)$: probabilidad de pasar de $s$ a $s'$ al aplicar $a$.                |\n",
    "| **Naturaleza de las transiciones** | Determinista (un solo destino posible).                   | Estocástica (transiciones con distribución de probabilidad).                                 |\n",
    "| **Salida o resultado**             | Acepta o rechaza la cadena (sí/no).                       | Devuelve recompensas y busca maximizar su valor esperado.                                    |\n",
    "| **Criterio de evaluación**         | Pertenencia de la cadena al lenguaje reconocido $L(A)$. | Valor esperado de la suma de recompensas (función de valor).                                 |\n",
    "| **Evolución temporal**             | Basada en una secuencia fija de símbolos de entrada.      | Basada en la interacción agente–entorno en pasos sucesivos.                                  |\n",
    "| **Control o decisión**             | Sin control (la entrada está dada).                       | El agente elige la acción en cada estado.                                                    |\n",
    "| **Probabilidades**                 | No existen (transiciones deterministas).                  | Incorporadas en $P(s' \\mid s,a)$.                                                          |\n",
    "| **Recompensas**                    | No hay noción de recompensa.                              | $R(s,a)$: recompensa esperada asociada a cada acción.                                      |\n",
    "| **Tipo de problema**               | Reconocimiento de lenguajes formales.                     | Aprendizaje por refuerzo, optimización secuencial, planificación.                            |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
