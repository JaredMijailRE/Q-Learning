{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6420f7",
   "metadata": {},
   "source": [
    "### Reinforcment Learning\n",
    "\n",
    "El aprendizaje por refuerzo, es una rama del **aprendizaje automatica**. Estudia **agentes** que deben aprender a tomar **decisiones secuenciales** en un **entorno estocastico**, con el objetivo de maximizar una **recompensa acumulada** a largo plazo.\n",
    "\n",
    "Finalmente RL se modelo de forma formal como un MDP (Markov Decision Process)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cabd4",
   "metadata": {},
   "source": [
    "#### Secuencia de eventos en RL\n",
    "\n",
    "1. El agente observa el estado $s_0$ del entorno.\n",
    "2. Elige una accion $a_0$ basada en su politica actual $\\pi$ (por ejemplo, una politica epsilon-greedy).\n",
    "3. La accion $a_0$ es ejecutada en el entorno.\n",
    "4. El agente observa el nuevo estado $s_1$ segun la funcion $P_a(s,s') = s_1$.\n",
    "5. El agente recibe una recompensa $r_1$ segun la funcion $R_a(s,s') = r_1$.\n",
    "6. El agente actualiza su politica $\\pi$ para maximizar la recompensa futura esperada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c096b",
   "metadata": {},
   "source": [
    "### Q-Learning y MDP\n",
    "\n",
    "\"For any finite Markov decision process, Q-learning finds an optimal policy, maximizing the expected value of the total reward over any and all successive steps, starting from the current state\".\n",
    "\n",
    "Es una aplicacion basada en un MDP, donde el agente aprende una politica optima sin conocer las funciones de transicion $P_a(s,s')$ y recompensa $R_a(s,s')$ del MDP. El agente aprende a traves de la interaccion con el entorno. Es una aplicacion especifica de RL. Pertenece a la familia off-policy.\n",
    "\n",
    "- El agente aprende una funcion de valor de accion $Q(s,a)$ (ecuacion de Bellman), que estima la recompensa futura esperada al tomar la accion $a$ en el estado $s$ y seguir la politica optima a partir de ahi.\n",
    "\n",
    "$$\n",
    "Q^*(s,a) = \\mathbb{E}\\big[ r_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') ,\\big|, s_t = s, a_t = a \\big]\n",
    "$$\n",
    "\n",
    "- $r_{t+1} :=$ recompensa inmediata\n",
    "\n",
    "El algoritmo actualiza una tabla ($Q(s,a)$):\n",
    "\n",
    "$$\n",
    "\\text{error temporal(TD err)} = r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t,a_t) = \\text{(realidad observada)-(valor previo)} = \\delta_t \n",
    "$$\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big[\\delta_t  \\big]\n",
    "$$\n",
    "\n",
    "- Pero esto solo funciona si los conjuntos (S) y (A) son **pequeños y discretos**, ya que guardar una tabla (Q) completa es inviable en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093bd69",
   "metadata": {},
   "source": [
    "###### intuicion: Q* es la funcion de politica optima que nos gustaria conocer, esta es el valor esperado mas la recompensa inmediata, mas las recompensas futuras esperadas con menos peso que el presente. El agente no conoce Q*, pero si conoce la recompensa inmediata y el valor de la siguiente accion, por lo que puede aproximar Q* con la ecuacion de TD error, que es basicamente el aposteriori y apriori restados. El agente actualiza su valor Q(s,a) con un paso de aprendizaje alpha, para acercarse a Q*. Con el tiempo y muchas iteraciones, Q(s,a) converge a Q*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be1d8e8",
   "metadata": {},
   "source": [
    "#### Off-policy vs On-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4d673",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Aspecto**                            | **On-policy**                                                                                                                                                    | **Off-policy**                                                                                                                                                          |\n",
    "| -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definición general**                 | El agente **aprende sobre la política que realmente ejecuta** en el entorno.                                                                                     | El agente **aprende sobre una política diferente** de la que usa para generar experiencias.                                                                             |\n",
    "| **Política de comportamiento** (($\\mu$)) | Es la **misma** que la política objetivo (($\\pi$)):  ($\\mu = \\pi$).                                                                                                  | Es **distinta** de la política objetivo:  ($\\mu \\neq \\pi$).                                                                                                               |\n",
    "| **Política objetivo (($\\pi$))**          | Se mejora usando los datos generados por ella misma.                                                                                                             | Se aprende una política óptima ($\\pi^*$) mientras se comporta de otra forma (por ejemplo, exploratoria).                                                                  |\n",
    "| **Exploración vs explotación**         | Debe equilibrar ambos aspectos dentro de la misma política (por ejemplo, usando ($\\epsilon$)-greedy).                                                              | Puede explorar libremente con una política exploratoria ($\\mu$), mientras aprende una política óptima determinista ($\\pi^*$).                                               |\n",
    "| **Ejemplo clásico**                    | SARSA (State–Action–Reward–State–Action).                                                                                                                        | Q-Learning.                                                                                                                                                             |\n",
    "| **Actualización típica**               | Usa la acción **realmente ejecutada** en el siguiente estado:  [ $Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)$] ] | Usa la **mejor acción posible** según el valor máximo estimado:  [ $Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t,a_t)$] ] |\n",
    "| **Consecuencia**                       | Aprende la política **real que sigue** el agente, lo que puede hacer el aprendizaje más estable pero menos óptimo.                                               | Aprende la política **óptima** incluso si el comportamiento durante el entrenamiento no es óptimo.                                                                      |\n",
    "| **Ventaja principal**                  | Mejor correlación entre datos y política aprendida (menor varianza).                                                                                             | Mayor eficiencia en el aprendizaje y mejor convergencia hacia la política óptima.                                                                                       |\n",
    "| **Desventaja principal**               | Puede quedarse atrapado en políticas subóptimas si la exploración no es suficiente.                                                                              | Mayor varianza y posible inestabilidad si la política exploratoria ($\\mu$) difiere mucho de ($\\pi$).                                                                        |\n",
    "| **Interpretación intuitiva**           | “Aprendo lo que hago.”                                                                                                                                           | “Aprendo lo que **debería** hacer, no necesariamente lo que **estoy** haciendo.”                                                                                        |\n",
    "\n",
    "\n",
    "* En *on-policy*, el aprendizaje sigue la distribución real de los datos.\n",
    "\n",
    "La expectativa de actualización se toma **bajo la misma distribución** de política:\n",
    "$$\n",
    "\\mathbb{E}_{s,a \\sim d^\\pi} [\\delta_t]\n",
    "$$\n",
    "\n",
    "* En *off-policy*, el aprendizaje requiere **corregir** o **reponderar** las muestras si se quiere una estimación no sesgada (a menudo mediante *importance sampling*).\n",
    "\n",
    "\n",
    "La expectativa de actualización se toma **bajo otra distribución de datos**:\n",
    "$$\n",
    "\\mathbb{E}_{s,a \\sim d^\\mu} [r + \\gamma \\mathbb{E}_{a'\\sim \\pi}[Q(s',a')] - Q(s,a)]\n",
    "$$\n",
    "\n",
    "donde ($d^\\mu(s,a)$) es la distribución estacionaria inducida por la política de comportamiento.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
